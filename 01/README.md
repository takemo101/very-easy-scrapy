# 01

Scrapy の基本的な部分  
※ mac で Python3 を利用する前提で進めますので、基本的な部分は自分で調べてね！

## pip で scrapy のインストール

pip で scrapy をインストールします。

```
pip3 install scrapy
```

最新は v2.5.0 になっていると思います。

## scrapy のプロジェクト作成

scrapy コマンドが利用できるようになっているので、コマンドでプロジェクトの作成や Spider の実行を行えます。

```
scrapy startproject jobbox
```

## scrapy でのクローラ作成の基本

scrapy でクローラを作成したら用意するものが基本的に 3 つあります。  
Spider クラスと Item クラスと Pipeline クラスです。プロジェクトを作成したら Item と Pipeline クラスについては、モックが jobbox/items.py と jobbox/pipelines.py として生成されていますが、Spider については自分で作成する必要があります。  
jobbox ディレクトリに移動して、以下コマンドで作成してみてください。

```
scrapy genspider jobbox_spider xn--pckua2a7gp15o89zb.com
```

実行すると spiders ディレクトリ以下に Spider クラスが生成されています。  
Spider クラスでのクロールについてはソースを見ながら調べてやってみてください。

### Spider とは

Spider はクロールでのリクエストとレスポンス処理を行うクラスになります。Spider クラスにレスポンスごとにメソッドを定義して、メソッド内でスクレイピング処理を行います。スクレイピング処理したら、次のリクエストをジェネレータ（yield）として返して、また別のメソッドにレスポンスを受け取る処理を記述します。最終的にはスクレイピングしたデータをメソッドから Item オブジェクトに詰めて yield で返すことで、Item オブジェクトが Pipeline に引き渡されることになります（最後に Item オブジェクトを返す必要がある）  
リクエストとレスポンスを Spider のメソッドで数珠つなぎにするようなイメージです。もしも途中でリクエストを止めたい場合は return で void を返します。  
Spider は一つのプロジェクトに幾つ作っても良い。

### Item とは

Spider クラスでスクレイピングしたデータを Item に詰めて yield で返すことで Pipeline 引き渡す役割です。つまり Spider から Pipeline に向けての DTO となります。

### Pipeline とは

スクレイピングしたデータを Item として受け取ります。このクラス内でスクレイピングしたデータを DB に保存したり、ファイルに出力したりします。Pipeline も複数用意して、Pipeline ごとに DB 処理したり通知処理したりということもできる。

## settings.py について

settings.py には、リクエストやミドルウェア、キャッシュ、クッキーなどのクロールに関する設定をすることができます。これらの設定は環境変数ですることもでき、基本的に環境変数が優先されます。

## クロールの実行

Spider にクロール処理を追加したら、以下コマンドで早速クロールを実行してみましょう！

```
scrapy crawl jobbox_spider
```

取得したデータが表示されていると思います。  
キャッシュはどうするか？とかデバッグは？とかはあると思いますが、細かい部分は今回説明しません。
